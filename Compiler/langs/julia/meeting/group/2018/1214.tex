\subsection{2018.12.14组会}
\label{sec:g2018:1019}
\bhei{出席人员}: \ZY, \ZhangLF, \CaiWT, \DongH, \HeLY, \YuCD, \DaiL, \HeJY
\newline
\bhei{记录}:\DaiL
\newline

\sect{工作情况}
\begin{itemize}
\item \YuCD:
Julia的语言层IR优化以及如何编写高效的Julia代码 

\item \DongH:
评测RLlib性能，分析Tune库，了解Ray中的RL算法，调查对RLlib的优化即RLgraph

\item \CaiWT:
Julia从AST到汇编

\item \HeLY:
初步探究julia的性能、julia编译流程

\item \HeJY：
Julia Channel,Task自顶向下分析,Julia源码编译

\item \ZhangLF:
调研了 Julia 的 Thread 部分的实现，介绍了 Thread-local storage (TLS) 在 Julia 中的实现方式与在线程处理过程中的作用，
对于 Thread 的初始化方式与 Thread 中函数的调用方式进行了相关介绍，后续计划对 Task 和 Thread 的实现进行相关调研。

\item \DaiL:
调研了julia编译在IR, AST层面与GPU compiler, GPU runtime之间的交互过程


\end{itemize}

\sect{下步工作建议(\ZY) } 
\begin{enumerate}[T1.]
\item \CaiWT、\HeLY、\YuCD 阅读\cite{fischer2018juliaMLtoTPU}(\href{https://arxiv.org/abs/1810.09868}{arXiv1810})并写笔记(中英文要点)到\S\ref{paper:arxiv1810JuliaMLtoTPU};
    \\Julia=>XLA, \href{http://arxiv.org/abs/1409.1556}{VGG19}(ICLR2015\href{https://www.iclr.cc/archive/www/lib/exe/fetch.php%3Fmedia=iclr2015:simonyan-iclr2015.pdf}{slides},\href{https://youtu.be/OQe-9P51Z0s}{video}):forward pass - \href{https://github.com/FluxML/Metalhead.jl}{Metalhead.jl}, backward pass \href{https://github.com/FluxML/Zygote.jl}{Zygote.jl}
\item \DaiL、\HeJY 阅读\cite{innes2018diffZygote}(\href{https://arxiv.org/abs/1810.07951}{arXiv1810})并写笔记(中英文要点)到\S\ref{paper:arxiv1810diffZygote}, \href{https://github.com/FluxML/Zygote.jl}{Zygote.jl} - source-to-source automatic differentiation (AD) in Julia;
%    \item 用Julia构建ML模型,如图像处理或语音识别;
\item \HeLY、\YuCD 测试评估v1.0和v1.0.2
\item \CaiWT、\HeLY、\YuCD Julia编译运行机制：类型推断、类型特化
\item \HeJY 进一步调研：1）libuv.jl是人工写的，还是对jl\_uv.c采取某种方式来 wrapper的；2）Julia支持C/C++扩展的约定？3）Channel.jl被使用的情况及其分析
\item \ZhangLF 调研threads被上层使用的情况，结合应用进行分析
\end{enumerate}

